[{"content":"introduction machine learning models can only understand data that be encoded numerical form real numbers such as distances mass population etc simply normalized fed algorithm here\u0026rsquos list of type most standard way encoding them pun intended method ratio normalize interval label one-hot encode categorical ordinal nominal images across channels words graphs edge index node features but things aren\u0026rsquot so straight forward for some example how would you date traditionally extracted from dates have been used weekend weekday season quarter one notice this lead lot information loss do we encapsulate entire given consider 2021-01-12 should year month day separate or why is bad idea periodicity days months years not captured by neural nets non-periodic activation functions exploding vanishing gradients due growing inputs time2vec say \\\\tau\\ \\in \\mathbb{r}^d\\ periodic input variable which want learn vector representation \\v \\mathbb{r}^{k}\\ layer defined where \\\\cal f\\ function e.g \\sine\\sdot\\ \\cosine\\sdot\\ \\w_i \\varphi_{i}\\ are learnable parameters points note \\k-1\\ components \\1 \\le k\\ component allows \\i 0\\ link paper code we\u0026rsquoll use pytorrch my open-source implementation pytorch think self-explanatory also provided pretrained model date-time iso format 132330 2019-7-23 64 dimensions pretraining task next prediction dataset pairs their immediate generated trained supervised fashion after training decoder discarded it now generate embeddings sample getting embedding shown below conclusion talked about incredibly simple technique","date":"2021-02-14","description":"Article that showcases the paper 'Time2Vec: Learning a Vector Representation of Time'","permalink":"https://ojus1.github.io/posts/time2vec/","tags":["periodicity","sine","cosine","vector","vanishing gradients"],"title":"Vectorizing Time with Time2Vec"}]